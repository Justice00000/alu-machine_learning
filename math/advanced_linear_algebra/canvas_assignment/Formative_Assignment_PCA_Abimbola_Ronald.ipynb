{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyATLU4z1cYj"
      },
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "outputs": [],
      "source": [
        "#import necessary package\n",
        "#TO DO\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "outputs": [],
      "source": [
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2U2_Q5ebos3"
      },
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "- **Answer:** Standardization ensures that each feature contributes equally to the principal components, preventing features with relatively larger magnitudes from dominating the analysis, which can lead to biased results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JF3eGB7FRC0A"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.36438208,  0.70710678,  1.5109662 , -0.99186978,  0.77802924],\n",
              "       [ 0.12403473, -1.94454365, -0.13736056,  0.77145428, -2.06841919],\n",
              "       [-0.62017367,  0.1767767 ,  0.68680282, -0.99186978,  0.20873955],\n",
              "       [ 1.61245155,  0.1767767 , -1.78568733,  0.33062326,  0.20873955],\n",
              "       [-0.62017367,  1.23743687, -0.13736056, -0.77145428,  1.00574511],\n",
              "       [ 0.86824314, -0.35355339, -0.13736056,  1.65311631, -0.13283426]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "standardized_data = scaler.fit_transform(data)\n",
        "standardized_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      },
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuhux3UEcBgw"
      },
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "cfa91ded-287c-488f-c7d4-7ce430ea66cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.2        -0.42098785 -1.0835838   0.90219291 -0.37000528]\n",
            " [-0.42098785  1.2         0.20397003 -0.77149364  1.18751836]\n",
            " [-1.0835838   0.20397003  1.2        -0.59947269  0.22208218]\n",
            " [ 0.90219291 -0.77149364 -0.59947269  1.2        -0.70017993]\n",
            " [-0.37000528  1.18751836  0.22208218 -0.70017993  1.2       ]]\n"
          ]
        }
      ],
      "source": [
        "cov_matrix = np.cov(standardized_data, rowvar=False)\n",
        "\n",
        "print(cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXNcG4AFcT08"
      },
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dmGlQ47tRO5w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3.80985761e+00 1.73655615e+00 4.94531029e-02 4.74189469e-05\n",
            " 4.04085720e-01]\n",
            "[[-0.4640131   0.45182808 -0.70733581  0.28128049 -0.03317471]\n",
            " [ 0.45019005  0.48800851  0.29051532  0.6706731  -0.15803498]\n",
            " [ 0.37929082 -0.55665017 -0.48462321  0.24186072 -0.5029143 ]\n",
            " [-0.4976889   0.03162214  0.36999674 -0.03373724 -0.78311558]\n",
            " [ 0.43642295  0.49682965 -0.20861365 -0.64143906 -0.32822489]]\n"
          ]
        }
      ],
      "source": [
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "print(eigenvalues)\n",
        "print(eigenvectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pWho88fcbJA"
      },
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "823e9389-f08c-45a1-c181-1cf0589a2a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the order of importance is :\n",
            " [0 1 4 2 3]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[3.80985761e+00 1.73655615e+00 4.04085720e-01 4.94531029e-02\n",
            " 4.74189469e-05]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.4640131   0.45182808 -0.03317471 -0.70733581  0.28128049]\n",
            " [ 0.45019005  0.48800851 -0.15803498  0.29051532  0.6706731 ]\n",
            " [ 0.37929082 -0.55665017 -0.5029143  -0.48462321  0.24186072]\n",
            " [-0.4976889   0.03162214 -0.78311558  0.36999674 -0.03373724]\n",
            " [ 0.43642295  0.49682965 -0.32822489 -0.20861365 -0.64143906]]\n"
          ]
        }
      ],
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance]  # sort the columns\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1nILNGxpTJB"
      },
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "We order eigenvalues and eigenvectors in order to focus on the components that capture the most variance, making PCA both more informative and efficient for dimensionality reduction.\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "No, we typically would not consider the lowest eigenvalue compared to the highest in Principal Component Analysis (PCA). Larger eigenvalues indicate that the corresponding principal components capture more of the variance in the data, representing more important features. Remember that the goal of PCA is to maximize the variance and retain as much information as possible, so we focus on the principal components with the highest eigenvalues, as they contribute the most to the data's structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWqFGNeNvgEB"
      },
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "fc22f63d-fa30-4c56-b849-829e6f21b7d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['63.50%', '28.94%', '6.73%', '0.82%', '0.00%']\n"
          ]
        }
      ],
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "#TO DO: Insert code here\n",
        "explained_variance = sorted_eigenvalues / np.sum(sorted_eigenvalues) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB7H4InbfKx5"
      },
      "source": [
        "## Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "outputs": [],
      "source": [
        "k = 2\n",
        "top_k_eigenvectors = eigenvectors[:, :k]\n",
        "reduced_data = np.matmul(standardized_data, top_k_eigenvectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "76eb09ad-ace9-4d15-bb2b-59b67d8092fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 2.3577116  -0.75728867]\n",
            " [-2.27171739 -1.81970663]\n",
            " [ 1.21259114 -0.50390931]\n",
            " [-1.41935914  1.9229856 ]\n",
            " [ 1.61562536  0.87541857]\n",
            " [-1.49485157  0.28250044]]\n"
          ]
        }
      ],
      "source": [
        "print(reduced_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "78e448df-45c8-45d7-dbfb-6bf810f2a486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 2)\n"
          ]
        }
      ],
      "source": [
        "print(reduced_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxQ8lTunauMQ"
      },
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "### **BENEFITS:**\n",
        "- PCA helps to reduce the number of features used in training our model which in turn, reduces computation time and resources.\n",
        "- PCA helps us focus more on components with most variance and filter out less important components which reduces noise in our training data therby improving the data quality.\n",
        "\n",
        "### **LIMITATIONS:**\n",
        "- Sometimes, we end up discarding too many dimensions/features with low variance which may lead to loss of important information.\n",
        "- It introduces the complexity of understanding what each of the newly engineered features represents which may make the data harder to interpret."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
